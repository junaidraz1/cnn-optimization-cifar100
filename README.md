# ğŸ§  #CIFAR100 #CNN #Experiments

This project focuses on #imageclassification using a custom #ConvolutionalNeuralNetwork (#CNN) trained on a reduced subset of the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). The aim is to analyze the impact of architectural and training variations on classification performance, all within the limitations of a free #GoogleColab environment.

## ğŸ“Œ #ProjectGoals

- Build a CNN from scratch using #TensorFlow and #Keras
- Use a subset of CIFAR-100 (#8to12 classes with reduced samples per class)
- Evaluate and improve classification accuracy through systematic experimentation

## âš™ï¸ #KeyFeatures

- ğŸ”¹ #DataPreprocessing and subset extraction from CIFAR-100
- ğŸ”¹ Custom CNN architecture with convolutional, max-pooling, and dense layers
- ğŸ”¹ #Experiments with CNN depth, #activationfunctions, #optimizers, batch sizes, and more
- ğŸ”¹ Evaluation of #modelperformance on the CIFAR-100 test set

## ğŸ”¬ #Experiments

- Test the effect of increasing/decreasing CNN layers
- Experiment with #activationfunctions like #ReLU, #Sigmoid, and #Softmax
- Try different #optimizers and learning rates
- Evaluate the effect of batch sizes and epochs on training and accuracy

## ğŸ—‚ï¸ #Files

- `cnn_cifar100.ipynb`: Main #Colab notebook with code and experiments

## ğŸ”§ #TechStack

- #TensorFlow / #Keras
- #Python (#Jupyter / #Colab)
- #CIFAR100Dataset (subset)
- #ImageNetDataset (subset)
